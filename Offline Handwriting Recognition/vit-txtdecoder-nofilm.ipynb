{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0678d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samya\\Desktop\\lab\\lab_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\samya\\Desktop\\lab\\lab_env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "Testing (ViT+Decoder CTC-aux NO FiLM):   0%|          | 0/323 [00:00<?, ?it/s]C:\\Users\\samya\\AppData\\Local\\Temp\\ipykernel_20676\\4064350557.py:125: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  return Image.fromarray(out, mode=\"L\")\n",
      "c:\\Users\\samya\\Desktop\\lab\\lab_env\\Lib\\site-packages\\torch\\nn\\functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Testing (ViT+Decoder CTC-aux NO FiLM): 100%|██████████| 323/323 [01:55<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT: KEVIN | PRED: KEVIN\n",
      "GT: CLOTAIRE | PRED: CLOTAINE\n",
      "GT: LENA | PRED: LENA\n",
      "GT: JULES | PRED: JULES\n",
      "GT: CHERPIN | PRED: CHERPIN\n",
      "Test CER: 0.045521\n",
      "Test 1-CER (Char Acc): 0.954479\n",
      "Test ACC (Exact): 0.840006\n",
      "Test WER: 0.159994\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Offline Handwritten Text OCR — ViT Encoder + Transformer Decoder (CTC-aux, BOS-cond, NO FiLM)\n",
    "# - Preprocess: drop NaNs / 'unreadable', crop top-left 64x256, pad white\n",
    "# - Encoder: ViT over 8x8 patches (T=256 tokens)\n",
    "# - Decoder: Transformer decoder with cross-attention\n",
    "# - Stabilizers against collapse:\n",
    "#     * Auxiliary CTC head on encoder (joint loss: CE + 0.3*CTC)\n",
    "#     * Token dropout on decoder inputs (p=0.15) during training\n",
    "#     * Image-conditioned BOS (WITHOUT FiLM modulation)\n",
    "#     * Weight tying (decoder out <- embedding weight)\n",
    "# - Vocab: <pad>=0, <sos>=1, <eos>=2, then dataset chars\n",
    "# - Metrics: CER, 1-CER, ACC, WER\n",
    "# - Saves: loss_curve_vit_seq2seq_nofilm.png, checkpoint seq2seq_vit_ctc_nofilm.ckpt\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Metrics\n",
    "# ---------------------------\n",
    "\n",
    "def levenshtein_distance(s1: str, s2: str) -> int:\n",
    "    if len(s1) < len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "    previous = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            ins = previous[j + 1] + 1\n",
    "            dele = current[j] + 1\n",
    "            sub = previous[j] + (c1 != c2)\n",
    "            current.append(min(ins, dele, sub))\n",
    "        previous = current\n",
    "    return previous[-1]\n",
    "\n",
    "def compute_metrics(preds, truths):\n",
    "    total_chars, total_char_errs = 0, 0\n",
    "    total_words, total_word_errs = 0, 0\n",
    "    exact = 0\n",
    "    for gt, pr in zip(truths, preds):\n",
    "        dist = levenshtein_distance(gt, pr)\n",
    "        total_char_errs += dist\n",
    "        total_chars += len(gt)\n",
    "        total_words += 1\n",
    "        total_word_errs += int(gt != pr)\n",
    "        if gt == pr:\n",
    "            exact += 1\n",
    "    cer = (total_char_errs / total_chars) if total_chars > 0 else 0.0\n",
    "    one_minus_cer = 1.0 - cer\n",
    "    acc = (exact / total_words) if total_words > 0 else 0.0\n",
    "    wer = (total_word_errs / total_words) if total_words > 0 else 0.0\n",
    "    return cer, one_minus_cer, acc, wer\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset (keeps your preprocessing)\n",
    "# ---------------------------\n",
    "\n",
    "class HandwritingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    - CSV must have 'FILENAME', 'IDENTITY'\n",
    "    - Remove NaNs and label == 'unreadable'\n",
    "    - Convert to grayscale, crop top-left to 64x256, pad white\n",
    "    - Returns: image tensor [1, 64, 256], raw text string, filename\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path, images_dir, transform=None,\n",
    "                 crop_h=64, crop_w=256, char2idx=None):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if \"IDENTITY\" not in df.columns or \"FILENAME\" not in df.columns:\n",
    "            raise ValueError(f\"CSV {csv_path} must have columns 'FILENAME' and 'IDENTITY'\")\n",
    "        df = df.dropna(subset=[\"FILENAME\", \"IDENTITY\"]).copy()\n",
    "        df[\"IDENTITY\"] = df[\"IDENTITY\"].astype(str)\n",
    "        df = df[df[\"IDENTITY\"].str.strip().str.lower() != \"unreadable\"].reset_index(drop=True)\n",
    "\n",
    "        self.df = df\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "        ])\n",
    "        self.crop_h = crop_h\n",
    "        self.crop_w = crop_w\n",
    "\n",
    "        # Vocab: <pad>=0, <sos>=1, <eos>=2, then your chars\n",
    "        if char2idx is None:\n",
    "            chars = sorted(list({c for text in self.df[\"IDENTITY\"] for c in text}))\n",
    "            self.char2idx = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2}\n",
    "            for i, c in enumerate(chars, start=3):\n",
    "                self.char2idx[c] = i\n",
    "        else:\n",
    "            self.char2idx = char2idx\n",
    "        self.idx2char = {i:c for c,i in self.char2idx.items()}\n",
    "\n",
    "        # For CTC auxiliary: map only real characters (exclude specials) to [1..K], blank=0\n",
    "        self.ctc_blank = 0\n",
    "        self.ctc_chars = [c for c in self.char2idx.keys() if c not in [\"<pad>\", \"<sos>\", \"<eos>\"]]\n",
    "        self.ctc_char2idx = {c: i+1 for i, c in enumerate(sorted(self.ctc_chars))}\n",
    "        self.ctc_idx2char = {i+1:c for i, c in enumerate(sorted(self.ctc_chars))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _crop_pad_top_left(self, img: Image.Image) -> Image.Image:\n",
    "        arr = np.array(img.convert(\"L\"), dtype=np.uint8)\n",
    "        h, w = arr.shape[:2]\n",
    "        crop = arr[:min(h, self.crop_h), :min(w, self.crop_w)]\n",
    "        out = np.ones((self.crop_h, self.crop_w), dtype=np.uint8) * 255\n",
    "        out[:crop.shape[0], :crop.shape[1]] = crop\n",
    "        return Image.fromarray(out, mode=\"L\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.images_dir, row[\"FILENAME\"])\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        img = self._crop_pad_top_left(img)\n",
    "        img_t = self.transform(img)  # [1,64,256]\n",
    "        text = row[\"IDENTITY\"]\n",
    "        return img_t, text, row[\"FILENAME\"]\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        return [self.char2idx[\"<sos>\"]] + [self.char2idx[c] for c in text if c in self.char2idx] + [self.char2idx[\"<eos>\"]]\n",
    "\n",
    "    def encode_text_ctc(self, text):\n",
    "        # CTC targets are just raw characters (no sos/eos), mapped to 1..K\n",
    "        return [self.ctc_char2idx[c] for c in text if c in self.ctc_char2idx]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        imgs, texts, filenames = zip(*batch)\n",
    "        imgs = torch.stack(imgs, dim=0)  # [B,1,64,256]\n",
    "\n",
    "        # For CE (seq2seq)\n",
    "        seqs = [self.encode_text(t) for t in texts]\n",
    "        max_len = max(len(s) for s in seqs)\n",
    "        pad_idx = self.char2idx[\"<pad>\"]\n",
    "        dec_in, dec_tg, lengths = [], [], []\n",
    "        for s in seqs:\n",
    "            inp = s[:-1]  # includes <sos> ... last char\n",
    "            tgt = s[1:]   # ... up to <eos>\n",
    "            lengths.append(len(tgt))\n",
    "            dec_in.append(inp + [pad_idx]*(max_len-1-len(inp)))\n",
    "            dec_tg.append(tgt + [pad_idx]*(max_len-1-len(tgt)))\n",
    "        dec_in = torch.tensor(dec_in, dtype=torch.long)   # [B,L]\n",
    "        dec_tg = torch.tensor(dec_tg, dtype=torch.long)   # [B,L]\n",
    "        lengths = torch.tensor(lengths, dtype=torch.long) # [B]\n",
    "\n",
    "        # For CTC auxiliary\n",
    "        ctc_targets_list = [torch.tensor(self.encode_text_ctc(t), dtype=torch.long) for t in texts]\n",
    "        ctc_targets = torch.cat(ctc_targets_list) if len(ctc_targets_list) else torch.empty(0, dtype=torch.long)\n",
    "        ctc_target_lengths = torch.tensor([len(t) for t in ctc_targets_list], dtype=torch.long)\n",
    "\n",
    "        return imgs, dec_in, dec_tg, lengths, filenames, texts, ctc_targets, ctc_target_lengths\n",
    "\n",
    "# ---------------------------\n",
    "# Positional emb\n",
    "# ---------------------------\n",
    "\n",
    "class PositionalEncodingLearned(nn.Module):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        nn.init.trunc_normal_(self.pe, std=0.02)\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# ---------------------------\n",
    "# ViT Encoder\n",
    "# ---------------------------\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, img_h=64, img_w=256, patch=8, d_model=256, nhead=8, num_layers=4, dim_ff=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert img_h % patch == 0 and img_w % patch == 0\n",
    "        self.patch = patch\n",
    "        self.num_patches = (img_h // patch) * (img_w // patch)  # 8 * 32 = 256\n",
    "        patch_dim = (patch * patch) * 1  # 1 channel\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(patch_dim, d_model),\n",
    "            nn.LayerNorm(d_model)\n",
    "        )\n",
    "        self.pos = PositionalEncodingLearned(self.num_patches, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n",
    "                                                   dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        unfold = nn.Unfold(kernel_size=(self.patch, self.patch), stride=(self.patch, self.patch))\n",
    "        patches = unfold(x).transpose(1, 2)      # [B, T, patch_dim]\n",
    "        tokens = self.proj(patches)              # [B, T, d_model]\n",
    "        tokens = self.pos(tokens)\n",
    "        enc = self.encoder(tokens)               # [B, T, d_model]\n",
    "        return enc\n",
    "\n",
    "# ---------------------------\n",
    "# Transformer Decoder (BOS conditioning WITHOUT FiLM) with weight tying\n",
    "# ---------------------------\n",
    "\n",
    "class TXTDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len=64, d_model=256, nhead=8, num_layers=4, dim_ff=512, dropout=0.1, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos = PositionalEncodingLearned(max_len, d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n",
    "                                                   dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "        # Weight tying\n",
    "        self.out.weight = self.emb.weight\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "        # Image-conditioned BOS (NO FiLM)\n",
    "        self.bos_adapter = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, L, device):\n",
    "        return torch.triu(torch.full((L, L), float(\"-inf\"), device=device), diagonal=1)\n",
    "\n",
    "    def forward(self, memory, y_inp, img_global):\n",
    "        B, L = y_inp.shape\n",
    "        device = y_inp.device\n",
    "\n",
    "        # Token embeddings\n",
    "        tgt = self.emb(y_inp)                 # [B,L,D]\n",
    "\n",
    "        # Image-conditioned BOS (NO FiLM modulation)\n",
    "        tgt[:, 0, :] = tgt[:, 0, :] + self.bos_adapter(img_global)\n",
    "        \n",
    "        tgt = self.pos(tgt)\n",
    "\n",
    "        tgt_mask = self._generate_square_subsequent_mask(L, device)  # [L,L]\n",
    "        tgt_key_padding_mask = (y_inp == self.pad_idx)                # [B,L]\n",
    "\n",
    "        out = self.decoder(tgt, memory,\n",
    "                           tgt_mask=tgt_mask,\n",
    "                           tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        logits = self.out(out)  # [B,L,V]\n",
    "        return logits\n",
    "\n",
    "# ---------------------------\n",
    "# Lightning Module with CTC auxiliary + token dropout\n",
    "# ---------------------------\n",
    "\n",
    "class ViTSeq2SeqOCR(pl.LightningModule):\n",
    "    def __init__(self, dataset: HandwritingDataset, d_model=256, nhead=8, enc_layers=4, dec_layers=4,\n",
    "                 dim_ff=512, dropout=0.1, lr=1e-3, max_decode_len=40,\n",
    "                 token_dropout_p=0.15, ctc_weight=0.3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['dataset'])\n",
    "        self.pad_idx = dataset.char2idx[\"<pad>\"]\n",
    "        self.sos_idx = dataset.char2idx[\"<sos>\"]\n",
    "        self.eos_idx = dataset.char2idx[\"<eos>\"]\n",
    "        self.vocab_size = len(dataset.char2idx)\n",
    "        self.idx2char = dataset.idx2char\n",
    "\n",
    "        # For CTC\n",
    "        self.ctc_blank = dataset.ctc_blank\n",
    "        self.ctc_char2idx = dataset.ctc_char2idx\n",
    "        self.ctc_vocab = 1 + len(self.ctc_char2idx)  # blank + chars\n",
    "\n",
    "        self.encoder = ViTEncoder(img_h=64, img_w=256, patch=8, d_model=d_model,\n",
    "                                  nhead=nhead, num_layers=enc_layers, dim_ff=dim_ff, dropout=dropout)\n",
    "        self.decoder = TXTDecoder(vocab_size=self.vocab_size, max_len=256, d_model=d_model,\n",
    "                                  nhead=nhead, num_layers=dec_layers, dim_ff=dim_ff,\n",
    "                                  dropout=dropout, pad_idx=self.pad_idx)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.pad_idx, label_smoothing=0.1)\n",
    "        self.ctc_head = nn.Linear(d_model, self.ctc_vocab)  # logits over CTC chars (blank+chars)\n",
    "        self.ctc_loss = nn.CTCLoss(blank=self.ctc_blank, zero_infinity=True)\n",
    "\n",
    "        self.max_decode_len = max_decode_len\n",
    "        self.token_dropout_p = token_dropout_p\n",
    "        self.ctc_weight = ctc_weight\n",
    "\n",
    "        # logging\n",
    "        self.train_epoch_losses, self.val_epoch_losses = [], []\n",
    "        self._train_buf, self._val_buf = [], []\n",
    "        self.val_preds, self.val_truths = [], []\n",
    "\n",
    "    def apply_token_dropout(self, y_inp):\n",
    "        if not self.training or self.token_dropout_p <= 0.0:\n",
    "            return y_inp\n",
    "        # drop only non-special tokens\n",
    "        y = y_inp.clone()\n",
    "        mask = (y != self.pad_idx) & (y != self.sos_idx) & (y != self.eos_idx)\n",
    "        drop = (torch.rand_like(y.float()) < self.token_dropout_p) & mask\n",
    "        y[drop] = self.pad_idx\n",
    "        return y\n",
    "\n",
    "    def forward(self, imgs, y_inp):\n",
    "        memory = self.encoder(imgs)         # [B,T,D]\n",
    "        img_global = memory.mean(dim=1)     # [B,D]\n",
    "        y_inp = self.apply_token_dropout(y_inp)\n",
    "        logits = self.decoder(memory, y_inp, img_global)  # [B,L,V]\n",
    "        # CTC head over encoder tokens\n",
    "        ctc_logits = self.ctc_head(memory)  # [B,T,C_ctc]\n",
    "        return logits, ctc_logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, dec_in, dec_tg, lengths, _, _, ctc_targets, ctc_target_lengths = batch\n",
    "        logits, ctc_logits = self(imgs, dec_in)\n",
    "\n",
    "        # CE loss\n",
    "        ce = self.criterion(logits.reshape(-1, logits.size(-1)), dec_tg.reshape(-1))\n",
    "\n",
    "        # CTC loss\n",
    "        # ctc_logits: [B,T,C] -> [T,B,C] log_probs\n",
    "        B, T, C = ctc_logits.shape\n",
    "        logp = ctc_logits.log_softmax(dim=-1).permute(1,0,2)  # [T,B,C]\n",
    "        input_lengths = torch.full((B,), T, dtype=torch.long, device=logp.device)\n",
    "        if ctc_targets.numel() == 0:\n",
    "            ctc = torch.tensor(0.0, device=logp.device)\n",
    "        else:\n",
    "            ctc = self.ctc_loss(logp, ctc_targets, input_lengths, ctc_target_lengths)\n",
    "\n",
    "        loss = ce + self.ctc_weight * ctc\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True, batch_size=imgs.size(0))\n",
    "        self.log(\"train_ce\", ce, prog_bar=False, on_epoch=True, batch_size=imgs.size(0))\n",
    "        self.log(\"train_ctc\", ctc, prog_bar=False, on_epoch=True, batch_size=imgs.size(0))\n",
    "        self._train_buf.append(loss.detach().cpu().item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, dec_in, dec_tg, lengths, filenames, raw_texts, ctc_targets, ctc_target_lengths = batch\n",
    "        logits, ctc_logits = self(imgs, dec_in)\n",
    "\n",
    "        ce = self.criterion(logits.reshape(-1, logits.size(-1)), dec_tg.reshape(-1))\n",
    "        B, T, C = ctc_logits.shape\n",
    "        logp = ctc_logits.log_softmax(dim=-1).permute(1,0,2)\n",
    "        input_lengths = torch.full((B,), T, dtype=torch.long, device=logp.device)\n",
    "        if ctc_targets.numel() == 0:\n",
    "            ctc = torch.tensor(0.0, device=logp.device)\n",
    "        else:\n",
    "            ctc = self.ctc_loss(logp, ctc_targets, input_lengths, ctc_target_lengths)\n",
    "\n",
    "        loss = ce + self.ctc_weight * ctc\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True, batch_size=imgs.size(0))\n",
    "        self.log(\"val_ce\", ce, prog_bar=False, on_epoch=True, batch_size=imgs.size(0))\n",
    "        self.log(\"val_ctc\", ctc, prog_bar=False, on_epoch=True, batch_size=imgs.size(0))\n",
    "        self._val_buf.append(loss.detach().cpu().item())\n",
    "\n",
    "        # Greedy decode for metrics\n",
    "        preds = self.greedy_decode(imgs, max_len=min(self.max_decode_len, dec_in.size(1)+5))\n",
    "        self.val_preds.extend(preds)\n",
    "        self.val_truths.extend(list(raw_texts))\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            for i in range(min(3, len(preds))):\n",
    "                self.print(f\"VAL SAMPLE {filenames[i]} → pred: {preds[i]} | gt: {raw_texts[i]}\")\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        if self._train_buf:\n",
    "            self.train_epoch_losses.append(float(np.mean(self._train_buf)))\n",
    "            self._train_buf = []\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self._val_buf:\n",
    "            self.val_epoch_losses.append(float(np.mean(self._val_buf)))\n",
    "            self._val_buf = []\n",
    "        if self.val_preds:\n",
    "            cer, one_minus_cer, acc, wer = compute_metrics(self.val_preds, self.val_truths)\n",
    "            self.log('val_CER', cer, prog_bar=True)\n",
    "            self.log('val_1_minus_CER', one_minus_cer, prog_bar=True)\n",
    "            self.log('val_ACC', acc, prog_bar=True)\n",
    "            self.log('val_WER', wer, prog_bar=True)\n",
    "            self.val_preds, self.val_truths = [], []\n",
    "\n",
    "    def on_fit_end(self):\n",
    "        if self.train_epoch_losses:\n",
    "            plt.figure(figsize=(6,4))\n",
    "            plt.plot(range(1, len(self.train_epoch_losses)+1), self.train_epoch_losses, label=\"Train Loss\")\n",
    "            if self.val_epoch_losses:\n",
    "                plt.plot(range(1, len(self.val_epoch_losses)+1), self.val_epoch_losses, label=\"Val Loss\")\n",
    "            plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss (CE + λ·CTC)\"); plt.title(\"ViT+Decoder (CTC-aux, NO FiLM) Loss\")\n",
    "            plt.legend(); plt.tight_layout()\n",
    "            plt.savefig(\"loss_curve_vit_seq2seq_nofilm.png\")\n",
    "            self.print(\"[Saved] loss_curve_vit_seq2seq_nofilm.png\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def greedy_decode(self, imgs, max_len=40):\n",
    "        self.eval()\n",
    "        device = imgs.device\n",
    "        memory = self.encoder(imgs)           # [B,T,D]\n",
    "        img_global = memory.mean(dim=1)       # [B,D]\n",
    "\n",
    "        B = imgs.size(0)\n",
    "        ys = torch.full((B, 1), self.sos_idx, dtype=torch.long, device=device)\n",
    "        finished = torch.zeros(B, dtype=torch.bool, device=device)\n",
    "        out_texts = [\"\"] * B\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            logits = self.decoder(memory, ys, img_global)  # [B,L,V]\n",
    "            next_logit = logits[:, -1, :]\n",
    "            next_tok = next_logit.argmax(dim=-1)\n",
    "\n",
    "            for i in range(B):\n",
    "                if finished[i]: \n",
    "                    continue\n",
    "                tok = next_tok[i].item()\n",
    "                if tok == self.eos_idx:\n",
    "                    finished[i] = True\n",
    "                elif tok not in (self.pad_idx, self.sos_idx):\n",
    "                    out_texts[i] += self.idx2char.get(tok, \"\")\n",
    "            ys = torch.cat([ys, next_tok.unsqueeze(1)], dim=1)\n",
    "            if finished.all():\n",
    "                break\n",
    "        return out_texts\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "# ---------------------------\n",
    "# Training / Testing harness\n",
    "# ---------------------------\n",
    "\n",
    "def make_loaders():\n",
    "    train_ds = HandwritingDataset(\n",
    "        r\"C:\\Users\\samya\\Desktop\\lab\\lab\\Offline Handwriting Recognition\\data\\handwriting_recognition\\written_name_train_v2.csv\",\n",
    "        r\"C:\\Users\\samya\\Desktop\\lab\\lab\\Offline Handwriting Recognition\\data\\handwriting_recognition\\train_v2\\train\",\n",
    "        crop_h=64, crop_w=256\n",
    "    )\n",
    "    val_ds = HandwritingDataset(\n",
    "        r\"C:\\Users\\samya\\Desktop\\lab\\lab\\Offline Handwriting Recognition\\data\\handwriting_recognition\\written_name_validation_v2.csv\",\n",
    "        r\"C:\\Users\\samya\\Desktop\\lab\\lab\\Offline Handwriting Recognition\\data\\handwriting_recognition\\validation_v2\\validation\",\n",
    "        crop_h=64, crop_w=256,\n",
    "        char2idx=train_ds.char2idx\n",
    "    )\n",
    "    test_ds = HandwritingDataset(\n",
    "        r\"C:\\Users\\samya\\Desktop\\lab\\lab\\Offline Handwriting Recognition\\data\\handwriting_recognition\\written_name_test_v2.csv\",\n",
    "        r\"C:\\Users\\samya\\Desktop\\lab\\lab\\Offline Handwriting Recognition\\data\\handwriting_recognition\\test_v2\\test\",\n",
    "        crop_h=64, crop_w=256,\n",
    "        char2idx=train_ds.char2idx\n",
    "    )\n",
    "\n",
    "    # FIXED: Set num_workers=0 for Windows compatibility\n",
    "    # This prevents DataLoader hanging issues on Windows during multiprocessing\n",
    "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=0,\n",
    "                              pin_memory=True, collate_fn=train_ds.collate_fn)\n",
    "    val_loader = DataLoader(val_ds, batch_size=128, shuffle=False, num_workers=0,\n",
    "                            pin_memory=True, collate_fn=val_ds.collate_fn)\n",
    "    test_loader = DataLoader(test_ds, batch_size=128, shuffle=False, num_workers=0,\n",
    "                             pin_memory=True, collate_fn=test_ds.collate_fn)\n",
    "    return train_ds, val_ds, test_ds, train_loader, val_loader, test_loader\n",
    "\n",
    "def train_vit_seq2seq():\n",
    "    # Set matmul precision for better Tensor Core utilization\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "    \n",
    "    train_ds, val_ds, test_ds, train_loader, val_loader, test_loader = make_loaders()\n",
    "    model = ViTSeq2SeqOCR(train_ds, d_model=256, nhead=8, enc_layers=4, dec_layers=4,\n",
    "                          dim_ff=512, dropout=0.1, lr=1e-3, max_decode_len=40,\n",
    "                          token_dropout_p=0.15, ctc_weight=0.3)\n",
    "    accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "    trainer = pl.Trainer(max_epochs=40, accelerator=accelerator, devices=1, log_every_n_steps=20)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    trainer.save_checkpoint(\"seq2seq_vit_ctc_nofilm.ckpt\")\n",
    "    return model, test_loader, test_ds\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_vit_seq2seq(model, test_loader, idx2char):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    all_preds, all_truths = [], []\n",
    "    for batch in tqdm(test_loader, desc=\"Testing (ViT+Decoder CTC-aux NO FiLM)\"):\n",
    "        imgs, dec_in, dec_tg, lengths, filenames, raw_texts, ctc_targets, ctc_target_lengths = batch\n",
    "        imgs = imgs.to(device)\n",
    "        preds = model.greedy_decode(imgs, max_len=dec_in.size(1)+5)\n",
    "        all_preds.extend(preds)\n",
    "        all_truths.extend(list(raw_texts))\n",
    "\n",
    "    for i in range(min(5, len(all_preds))):\n",
    "        print(f\"GT: {all_truths[i]} | PRED: {all_preds[i]}\")\n",
    "\n",
    "    cer, one_minus_cer, acc, wer = compute_metrics(all_preds, all_truths)\n",
    "    print(f\"Test CER: {cer:.6f}\")\n",
    "    print(f\"Test 1-CER (Char Acc): {one_minus_cer:.6f}\")\n",
    "    print(f\"Test ACC (Exact): {acc:.6f}\")\n",
    "    print(f\"Test WER: {wer:.6f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #model, test_loader, test_ds = train_vit_seq2seq()\n",
    "    # Optionally reload:\n",
    "    train_ds, val_ds, test_ds, train_loader, val_loader, test_loader = make_loaders()\n",
    "    model = ViTSeq2SeqOCR.load_from_checkpoint(\n",
    "        r\"C:\\Users\\samya\\Desktop\\lab\\lab\\Offline Handwriting Recognition\\lightning_logs\\version_2\\checkpoints\\epoch=17-step=46458.ckpt\", dataset=test_ds, d_model=256, nhead=8, enc_layers=4, dec_layers=4,\n",
    "        dim_ff=512, dropout=0.1, lr=1e-3, max_decode_len=40, token_dropout_p=0.15, ctc_weight=0.3\n",
    "    )\n",
    "    test_vit_seq2seq(model, test_loader, test_ds.idx2char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
